#custom-xgboost
Алгоритм xgboost_mlbears

Цель алгоритма:
 построение набора "слабых" неглубоких деревьев принятия решений по обучающей (маркированной) выборке.
 Под деревом принятия решения (DT) понимается такая тройка:
   само дерево как граф,
   критерий разделения в каждой нетерминальной вершине (фактор, по которому идет деление и значение для разделения)
   веса (scores) в каждом листе дерева

 На каждой итерации строится одно дерево.
 Построение каждого следующего дерева опирается на предыдущие деревья (исправляя их ошибки).
 Итоговая оценка: сумма оценок всех деревьев, то есть
  для каждого i=1,..,N: итоговое значение искомого y_i = сумме_по всем деревьям (значений весов листов, которым принадлежит данное x_i).

Глобальные заданные параметры
 N: количество записей в обучающей выборке
 d: количество факторов

Глобальные изменяемые параметры
 K: количество деревьев, которые строим
 T: максимальное количество листов в каждом дереве

Функция потерь:
 состоит из двух слагаемых:
  расстояние между предсказаниями и маркированными значениями (евклидово, логистическое или любое другое)
  функция сложности дерева (complexity).
  см. https://xgboost.readthedocs.io/en/latest/model.html разделы "Objective Function" и "Model Complexity"

Что необходимо строить, а что следует из построения:
 Для каждой конкретной пары (граф; критерий разделения в узлах), можно найти оптимальный (в смысле функции потерь) набор весов,
 и таким образом построить дерево принятия решений.
 Также можно найти некоторую оценку дерева. Для этого графу эквивалентно сопоставляется отображение из множества значений факторов
 в T-мерный вектор (T-количество листов).
 Оптимальные параметры и оценка строится также исходя из предыдущих деревьев, а именно исходя из "потерь" на предыдущей операции.
 см. https://xgboost.readthedocs.io/en/latest/model.html разделы "The Structure Score" и "Learn the tree structure"

Алгоритм:
 Цикл от 1 до К (строим очередное дерево)
  Каждое следующее дерево строится исходя из минимизации функции потерь, при помощи "жадного" алгоритма, который строит деревья начиная с корня:
  по всем текущим терминальным узлам
   в каждом узле по всем доступным в нем факторам
    упорядочиваем выборку узла по фактору и рассматриваем всевозможные разделения на два подмножества
    по всем возможные разделениям на два подмножества
     вычисляем "функцию потерь" разделения
  среди всех разделений выбираем наиболее выгодное
  останавливаемся, если нет выгодных разделений, либо
                   если исчерпали лимит листьев дерева (на каждом шаге кол-во листьев возрастает на 1).


Замечание:
 Таким образом, на каждом шаге имеем d*N переборов (N-размер обучающей выборки, d-количество факторов), т.е. для построения
 дерева из T листов необходимо T*d*N действия.
 Для построения K деревьев по T листов: K*T*d*N действий.

a)
The Structure Score¶
расчет весов для текущего дерева.
https://xgboost.readthedocs.io/en/latest/model.html


b) Objective Function : Training Loss + Regularization функция потерь

c) first step